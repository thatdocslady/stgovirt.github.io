<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8 lt-ie7" lang="en-us"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8" lang="en-us"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if IE 9]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if lt IE 10]> <html class="no-js lt-ie10" lang="en-us"> <![endif]-->
<!--[if !IE]> > <![endif]-->
<html class='no-js' lang='en'>
<!-- <![endif] -->
<head>
<title>
Storage - oVirt workshop November 2011 &mdash;
oVirt
</title>
<meta charset='utf-8'>
<meta content='' name='description'>
<meta content='' name='author'>
<meta content='initial-scale=1.0,user-scalable=no,maximum-scale=1,width=device-width' name='viewport'>

<link href='/images/favicon.ico' rel='shortcut icon'>
<link href='/images/apple-touch-icon-precomposed.png' rel='apple-touch-icon-precomposed'>
<link href='/images/apple-touch-icon-57x57-precomposed.png' rel='apple-touch-icon-precomposed' sizes='57x57'>
<link href='/images/apple-touch-icon-72x72-precomposed.png' rel='apple-touch-icon-precomposed' sizes='72x72'>
<link href='/images/apple-touch-icon-114x114-precomposed.png' rel='apple-touch-icon-precomposed' sizes='114x114'>
<link href="/stylesheets/application.css?1447687441" rel="stylesheet" type="text/css" />
<link href="/stylesheets/print.css?1447687441" rel="stylesheet" type="text/css" media="print" />
</head>
<body class=' source-md'>
<header class='masthead hidden-print' id='branding' role='banner'>
<section class='hgroup'>
<h1>
<a href="/"><img id="logo" class="logo is-svg" alt="oVirt" src="/images/logo.svg?1447687442" />
</a></h1>
</section>
<div id='access'>
<nav role='navigation'>
<ul class='nav nav-pills'>
<li class='nav-link-home' role='menuitem'>
<a href='/'>Home</a>
</li>

<li class='nav-link-download' role='menuitem'>
<a href='/download/'>Download</a>
</li>

<li class='nav-link-documentation' role='menuitem'>
<a href='/documentation/'>Documentation</a>
</li>

<li class='nav-link-community' role='menuitem'>
<a href='/community/'>Community</a>
</li>

<li class='nav-link-develop' role='menuitem'>
<a href='/develop/'>Develop</a>
</li>

<li class='nav-link-search' role='menuitem'>
<a href='/search/'>Search</a>
</li>

</ul>
</nav>

</div>
</header>

<section class='page-wrap' id='page-wrap'>
<section class='page' id='page'>
<section class='container content' id='content'>
<div class='alert alert-warning'>
<div class='pull-right'>
<a>Hover for more info</a>
</div>
<div class='frontmatter-metadata'>
<table class='metadata'>
<caption>Frontmatter Metadata</caption>
<tr>
<th>title</th>
<td>Storage - oVirt workshop November 2011</td>
</tr>
<tr>
<th>category</th>
<td>event/workshop</td>
</tr>
<tr>
<th>authors</th>
<td>dannfrazier, quaid</td>
</tr>
<tr>
<th>wiki_category</th>
<td>Workshop November 2011</td>
</tr>
<tr>
<th>wiki_title</th>
<td>Storage - oVirt workshop November 2011</td>
</tr>
<tr>
<th>wiki_revision_count</th>
<td>2</td>
</tr>
<tr>
<th>wiki_last_updated</th>
<td>2011-11-03</td>
</tr>
</table>
</div>
Original Wiki page:
<a href='http://ovirt.org/Storage_-_oVirt_workshop_November_2011' target='_blank'>
Storage - oVirt workshop November 2011
</a>
</div>

<!--[if lt IE 7]>
<p class="chromeframe">You are using an outdated browser.
<a href="http://browsehappy.com/">Upgrade your browser today</a> or
<a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
<![endif]-->
<p>When done, post these notes permanently on the wiki at <a href="wiki/Storage_-_oVirt_workshop_November_2011">http://www.ovirt.org/wiki/Storage_-_oVirt_workshop_November_2011</a></p>

<h1 id="storage">Storage</h1>

<ul>
  <li>Assumptions - disk image accessed only from one node (forget about shared images for now, will discuss later)</li>
  <li>Big images - not a general purpose fs</li>
  <li>centralized management (one node manages the rest, though that node can be HA)</li>
</ul>

<h1 id="guidelines">Guidelines</h1>

<ul>
  <li>High level, image centered API</li>
  <li>Clustersafe (though we don't currently protect against one node seeing a disk image from a VM running on another node)</li>
  <li>High performance (not in data path)</li>
  <li>Highly available
    <ul>
      <li>no single pof</li>
      <li>continues working in the absence of the manager</li>
    </ul>
  </li>
  <li>Backing store agnostic</li>
</ul>

<h3 id="storage-domain">Storage Domain</h3>

<ul>
  <li>Standalone entity</li>
  <li>Stores the images and assocated metadata (but not vms)</li>
  <li>Only true persistent storage for VDSM</li>
</ul>

<h3 id="domain-classes">Domain Classes</h3>

<ul>
  <li>Data
    <ul>
      <li>Master (will get to that)</li>
    </ul>
  </li>
  <li>ISO (NFS only) "abomination"</li>
  <li>Backup (NFS only): export domain - used to move disk images around. exists due to "wrong design decision" called storage pool</li>
  <li>Domain classes are planned for deprecation</li>
</ul>

<h3 id="domain-types">Domain Types</h3>

<ul>
  <li>File Domains (NFS, local dir) - will be lustre or other shared filesystems
    <ul>
      <li>Use file system features for segmentation</li>
      <li>Use file system for synchronizing access</li>
    </ul>
  </li>
  <li>Block Domains (iSCSI, FCP, FCoE, SAS, â€¦): we managed software iSCSI luns for you
    <ul>
      <li>Use LVM for segmentation</li>
      <li>Use reserved space to ensure writes do not overlap</li>
      <li>Very specialized use of LVM
        <ul>
          <li>we introduced clustering layer on top of it.</li>
          <li>unique to oVirt</li>
        </ul>
      </li>
      <li>Sparse files</li>
      <li>Better image manipulation capabilities
        <ul>
          <li>when we delete an image we guarnatee blocks are zero'd out (get that for free w/ fs-stored files)
            <ul>
              <li>we don't release until we zero; we don't pre-zero (VMWare does - performance hit while running for them, just while deleting for us)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Volumes and metadata are files</li>
      <li>1:1 Mapping between domain and dir / NFS export
        <ul>
          <li>Cannot span across multiple exports</li>
        </ul>
      </li>
      <li>NFS - Different error handling logic for data path and control path
        <ul>
          <li>When a VM issues IO to its disk over NFS I/O might hang.</li>
          <li>we use soft mounts (unorthodox)</li>
          <li>When VM gets an -EIO (hung mount) it suspends VM</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="block-domains">Block Domains</h1>

<ul>
  <li>Mail box</li>
  <li>Thin provisioning</li>
  <li>Slower image manipulation
    <ul>
      <li>Everything done in 1G allocations</li>
    </ul>
  </li>
  <li>Devices managed by device-mapper &amp; multipath</li>
  <li>Domain is a VG
    <ul>
      <li>store image on single LV</li>
    </ul>
  </li>
  <li>Metadata is stored in a single LV and in lvm tags- goes back to scalability issues w/ LVM. Minimal allocation size of a LV is 128 MB, can't create one for each to store metadata.</li>
  <li>Volumes are LVs
    <ul>
      <li>Use QCOW in files *and* block devices - directly over raw device</li>
    </ul>
  </li>
</ul>

<h1 id="storage-pool-will-be-deprecated">Storage Pool (will be deprecated)</h1>

<ul>
  <li>Original thinking is that we'd wantt o manage all of our disk images in a single place even if they span storage domains</li>
  <li>Can't mix storage pools in data centers</li>
</ul>

<p>Q: Can you tune nfs/fc settings/striping etc A: No. mount options are configurable at host level, will hopefully go into oVirt image</p>

<h1 id="master-domain">Master Domain</h1>

<ul>
  <li>Used to store:
    <ul>
      <li>Pool metadata</li>
      <li>Backup of OVFs (treated as blobs)</li>
      <li>Async tasks persistent data)</li>
      <li>Each QCOW layer is a separate volume on the VG</li>
    </ul>
  </li>
  <li>Contains the clustered lock for the pool
    <ul>
      <li>Safe lease mechanism used to prevent multiple masters; exists at pool level; only this host can create new volumes</li>
    </ul>
  </li>
</ul>

<p>= Storage Pool Manager (SPM) A role assigned to one host in a data center granting it sole authority over:</p>

<ul>
  <li>Creation, deletion, an dmanipulation of virtula disk images, snapshots and templates
    <ul>
      <li>Templates: you can create on VM as a golden image and provision to multiple VMs (QCOW layers)</li>
    </ul>
  </li>
  <li>Allocation of storage for sparse block devices (on SAN)
    <ul>
      <li>Thin provisinoing (see below)</li>
    </ul>
  </li>
  <li>Single metadata writer:
    <ul>
      <li>SPM lease mechanism (Chockler and Malkhi 2004, Light-Weight Leases for Storage-Cnntric Coordination)</li>
      <li>Storage-centric mailbox</li>
    </ul>
  </li>
  <li>This role can be migrated to any host in data center</li>
</ul>

<h1 id="thin-provisioning">Thin Provisioning</h1>

<p>Over-commitment is a storage function which allows oVirt to loically allocate more storage than is physically avialable.</p>

<ul>
  <li>Generally vitual machines use much less storage than what has been defined for them</li>
  <li>Over-commitment allows a vm to operate completely unaware of the resources that are actually available</li>
  <li>Uses QCOW over LVM block devices
    <ul>
      <li>QCOW will always allocate space sequentially. Once it has reached a certian threshold, we can behind teh scenes extend the logical volume. Monitored using polling; if we check too late, -ENOSPC to QEMU, VM automatically pauses in that case. Causes event to be sent to the monitor. Engine notifies SPM via a mailbox on another logical volume, SPM extends, returns response in mailbox, host refreshes device-mapper, then unpauses VM (if it was paused)</li>
      <li>Qemu supports API call that lets you know highest-offest written to on target device</li>
    </ul>
  </li>
</ul>

<p>Q: Why not use a filesystem? A: It doesn't work at these scales Q: why mailbox? A: only guaranteed communication mechanism across domains is storage</p>

<h1 id="roadmap">Roadmap</h1>

<ul>
  <li>SANLock: mechanism to take storage-centric leases, but not as slow as safeleases. sub-second, to take leases on disk images. Integrated into libvirt.
    <ul>
      <li>SANLock is a misnomer; will work over files</li>
    </ul>
  </li>
  <li>SDM: get rid of storage pool concept and have storage domain manager.
    <ul>
      <li>You will then be ablet o have a data center where you can mix/match storage domains</li>
    </ul>
  </li>
  <li>Live snapshots</li>
  <li>Live storage migration (block copy/streaming)</li>
  <li>Direct LUN - today there's no way for a VM to use this directly; patches should come soon</li>
  <li>Support any shared file system (not just NFS)</li>
  <li>NFS v4</li>
  <li>NFS Hardmounts</li>
  <li>Offload (snapshots, lun provisinion, thin provisioning, etc)</li>
  <li>We want to discover capabilities of storage (can they find better snapshotting, etc). Want to have a common linux layer for that e.g. libstorage (current name, could change)</li>
  <li>Image handling
    <ul>
      <li>Image Manager</li>
      <li>Allocation policy (Space/Performance)</li>
    </ul>
  </li>
  <li>Dynamic Connection Management</li>
  <li>Liveness monitoring through storage</li>
  <li>So much more</li>
</ul>

<p><a href="Category: Workshop November 2011">Category: Workshop November 2011</a></p>

</section>
</section>
</section>
<footer class='text-center' id='footer'>
<hr class='visible-print'>
<ul class='footer-nav-list'>
<li><a target="_blank" href="/privacy/">Privacy policy</a></li>
<li><a target="_blank" href="/about/">About</a></li>
<li><a target="_blank" href="/disclaimers/">Disclaimers</a></li>
</ul>

&copy; 2013&ndash;2015 oVirt
<div class='last-modified'>
Page last modified
Mon 22 Jun 2015 10:11 UTC
</div>
</footer>


<script src="/javascripts/application.js?1447687452" type="text/javascript"></script>

<!-- eloqua -->
<script src='http://www.redhat.com/j/elqNow/elqCfg.js' type='text/javascript'></script>
<script src='http://www.redhat.com/j/elqNow/elqImg.js' type='text/javascript'></script>

</body>
</html>
