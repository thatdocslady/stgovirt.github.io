<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8 lt-ie7" lang="en-us"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8" lang="en-us"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if IE 9]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if lt IE 10]> <html class="no-js lt-ie10" lang="en-us"> <![endif]-->
<!--[if !IE]> > <![endif]-->
<html class='no-js' lang='en'>
<!-- <![endif] -->
<head>
<title>
Phoenix Lab oVirt Hosts &mdash;
oVirt
</title>
<meta charset='utf-8'>
<meta content='' name='description'>
<meta content='' name='author'>
<meta content='initial-scale=1.0,user-scalable=no,maximum-scale=1,width=device-width' name='viewport'>

<link href='/images/favicon.ico' rel='shortcut icon'>
<link href='/images/apple-touch-icon-precomposed.png' rel='apple-touch-icon-precomposed'>
<link href='/images/apple-touch-icon-57x57-precomposed.png' rel='apple-touch-icon-precomposed' sizes='57x57'>
<link href='/images/apple-touch-icon-72x72-precomposed.png' rel='apple-touch-icon-precomposed' sizes='72x72'>
<link href='/images/apple-touch-icon-114x114-precomposed.png' rel='apple-touch-icon-precomposed' sizes='114x114'>
<link href="/stylesheets/application.css?1447609181" rel="stylesheet" type="text/css" />
<link href="/stylesheets/print.css?1447609181" rel="stylesheet" type="text/css" media="print" />
</head>
<body class=' source-md'>
<header class='masthead hidden-print' id='branding' role='banner'>
<section class='hgroup'>
<h1>
<a href="/"><img id="logo" class="logo is-svg" alt="oVirt" src="/images/logo.svg?1447609182" />
</a></h1>
</section>
<div id='access'>
<nav role='navigation'>
<ul class='nav nav-pills'>
<li class='nav-link-home' role='menuitem'>
<a href='/'>Home</a>
</li>

<li class='nav-link-download' role='menuitem'>
<a href='/download/'>Download</a>
</li>

<li class='nav-link-documentation' role='menuitem'>
<a href='/documentation/'>Documentation</a>
</li>

<li class='nav-link-community' role='menuitem'>
<a href='/community/'>Community</a>
</li>

<li class='nav-link-develop' role='menuitem'>
<a href='/develop/'>Develop</a>
</li>

<li class='nav-link-search' role='menuitem'>
<a href='/search/'>Search</a>
</li>

</ul>
</nav>

</div>
</header>

<section class='page-wrap' id='page-wrap'>
<section class='page' id='page'>
<section class='container content' id='content'>
<div class='alert alert-warning'>
<div class='pull-right'>
<a>Hover for more info</a>
</div>
<div class='frontmatter-metadata'>
<table class='metadata'>
<caption>Frontmatter Metadata</caption>
<tr>
<th>title</th>
<td>Phoenix Lab oVirt Hosts</td>
</tr>
<tr>
<th>category</th>
<td>infra</td>
</tr>
<tr>
<th>authors</th>
<td>dcaroest</td>
</tr>
<tr>
<th>wiki_category</th>
<td>Infrastructure</td>
</tr>
<tr>
<th>wiki_title</th>
<td>Infra/Phoenix Lab oVirt Hosts</td>
</tr>
<tr>
<th>wiki_revision_count</th>
<td>2</td>
</tr>
<tr>
<th>wiki_last_updated</th>
<td>2015-02-25</td>
</tr>
</table>
</div>
Original Wiki page:
<a href='http://ovirt.org/Infra/Phoenix_Lab_oVirt_Hosts' target='_blank'>
Infra/Phoenix Lab oVirt Hosts
</a>
</div>

<!--[if lt IE 7]>
<p class="chromeframe">You are using an outdated browser.
<a href="http://browsehappy.com/">Upgrade your browser today</a> or
<a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
<![endif]-->
<h1 id="phoenix-lab-ovirt-hosts">Phoenix Lab oVirt Hosts</h1>

<p>All the hosts have a server installation of Fedora 19, with a hardware RAID5 setup and bonding on all interfaces.</p>

<p>The hosts are separated in two groups, one that hosts the <a href="/howto/hosted-engine/">hosted engine</a> and all the others. Right now we also have one of the hosts reserved (ovirt-srv08.ovirt.org) for the new integration testing framework.</p>

<h2 id="network-configuration">Network configuration</h2>

<p>Due to an restriction of the bonding drivers on Fedora 19, the network bond interfaces has to be bond1 instead of bond0, so the relevant networking configuration files end up as:</p>

<pre class="highlight plaintext"><code>[root@ovirt-srv01 ~]# cat /etc/sysconfig/network&#x000A;HOSTNAME=ovirt-srv01&#x000A;GATEWAY=66.187.230.126&#x000A;[root@ovirt-srv01 ~]# cat /etc/sysconfig/network-scripts/ifcfg-em1&#x000A;# Generated by VDSM version 4.14.9-0.fc19&#x000A;DEVICE=em1&#x000A;ONBOOT=yes&#x000A;HWADDR=f8:bc:12:3b:4e:08&#x000A;NM_CONTROLLED=no&#x000A;SLAVE=yes&#x000A;MASTER=bond1&#x000A;USERCTL=no&#x000A;&#x000A;[root@ovirt-srv01 ~]# cat /etc/sysconfig/network-scripts/ifcfg-bond1&#x000A;DEVICE=bond1&#x000A;ONBOOT=yes&#x000A;BRIDGE=ovirtmgmt&#x000A;NM_CONTROLLED=no&#x000A;STP=no&#x000A;HOTPLUG=no&#x000A;BONDING_OPTS=&amp;quot;mode=4 miimon=100 lacp_rate=1&amp;quot;&#x000A;</code></pre>

<p>Note the special <em>BONDING_OPTS</em>, that sets the type of bonding and rate to the same configured in the switch.</p>

<p>The current network range that we are using is 66.187.230.0/25, with the gateway at 60.187.230.126. All the ips are public but there's a transparent firewall that blocks any incoming request.</p>

<h2 id="hosted-engine-management">Hosted engine management</h2>

<p>The three first hosts (when writing this), <code>ovirt-srv01</code>, <code>ovirt-srv02</code> and <code>ovirt-srv03</code> are the ones that manage the hosted engine vm. That hosted engine is not handled by itself but by a couple of services and scripts installed bu the hosted-engine rpms.</p>

<p>To check the current status of the hosted engine cluster, you can run from any of those hosts:</p>

<pre class="highlight plaintext"><code>[root@ovirt-srv01 ~]# hosted-engine --vm-status&#x000A;&#x000A;--== Host 1 status ==--&#x000A;&#x000A;Status up-to-date                  : True&#x000A;Hostname                           : 66.187.230.3&#x000A;Host ID                            : 1&#x000A;Engine status                      : {&amp;quot;health&amp;quot;: &amp;quot;good&amp;quot;, &amp;quot;vm&amp;quot;: &amp;quot;up&amp;quot;, &amp;quot;detail&amp;quot;: &amp;quot;up&amp;quot;}&#x000A;Score                              : 2400&#x000A;Local maintenance                  : False&#x000A;Host timestamp                     : 1415642612&#x000A;Extra metadata (valid at timestamp):&#x000A;    metadata_parse_version=1&#x000A;    metadata_feature_version=1&#x000A;    timestamp=1415642612 (Mon Nov 10 11:03:32 2014)&#x000A;    host-id=1&#x000A;    score=2400&#x000A;    maintenance=False&#x000A;    state=EngineUp&#x000A;&#x000A;--== Host 2 status ==--&#x000A;&#x000A;Status up-to-date                  : True&#x000A;Hostname                           : 66.187.230.4&#x000A;Host ID                            : 2&#x000A;Engine status                      : {&amp;quot;reason&amp;quot;: &amp;quot;vm not running on this host&amp;quot;, &amp;quot;health&amp;quot;: &amp;quot;bad&amp;quot;, &amp;quot;vm&amp;quot;: &amp;quot;down&amp;quot;, &amp;quot;detail&amp;quot;: &amp;quot;unknown&amp;quot;}&#x000A;Score                              : 2400&#x000A;Local maintenance                  : False&#x000A;Host timestamp                     : 1415642616&#x000A;Extra metadata (valid at timestamp):&#x000A;    metadata_parse_version=1&#x000A;    metadata_feature_version=1&#x000A;    timestamp=1415642616 (Mon Nov 10 11:03:36 2014)&#x000A;    host-id=2&#x000A;    score=2400&#x000A;    maintenance=False&#x000A;    state=EngineDown&#x000A;&#x000A;--== Host 3 status ==--&#x000A;&#x000A;Status up-to-date                  : True&#x000A;Hostname                           : ovirt-srv03.ovirt.org&#x000A;Host ID                            : 3&#x000A;Engine status                      : {&amp;quot;reason&amp;quot;: &amp;quot;vm not running on this host&amp;quot;, &amp;quot;health&amp;quot;: &amp;quot;bad&amp;quot;, &amp;quot;vm&amp;quot;: &amp;quot;down&amp;quot;, &amp;quot;detail&amp;quot;: &amp;quot;unknown&amp;quot;}&#x000A;Score                              : 2400&#x000A;Local maintenance                  : False&#x000A;Host timestamp                     : 1415642615&#x000A;Extra metadata (valid at timestamp):&#x000A;    metadata_parse_version=1&#x000A;    metadata_feature_version=1&#x000A;    timestamp=1415642615 (Mon Nov 10 11:03:35 2014)&#x000A;    host-id=3&#x000A;    score=2400&#x000A;    maintenance=False&#x000A;    state=EngineDown&#x000A;</code></pre>

<p>You can see that the engine is running only on one of the hosts. You can set one host into maintenance mode executing:</p>

<pre class="highlight plaintext"><code>[root@ovirt-srv01 ~]# hosted-engine --set-maintenance=local&#x000A;</code></pre>

<p>From the selected host. You can also handle the vm engine with hoset-endine command (<strong>don't do it through the engine ui</strong>).</p>

<h2 id="ovirt-datacenter-organization">oVirt datacenter organization</h2>

<p>All the hosts are distributed across two datacenters, jenkins and production (Default in the ui), the first one is ment to host all the jenkins related vms and any testing vm outside jenkins. The production one is configured for high availability and is ment to host all the service vms that host production services like foreman, jenkins or resources01.</p>

<h3 id="production-vms">Production VMs</h3>

<p>There are no templates yet in this datacenter but some base netinstall images are uploaded, so if you have to create a new vm you'd be able to do so using the netinstall boot from foreman.</p>

<p>Currently these are the vms that we have in the production datacenter:</p>

<ul>
  <li>foreman-phx: Foreman proxy serving the phoenix network, includes DHCP, TFTP and DNS services. Also serves (or will) as DNS for the network.</li>
  <li>HostedEngine: VM with the hosted engine, is not actually managed by itself but by the hosted engine services.</li>
  <li>resources01-phx-ovirt-org: Frontend to serve the old repositories in resources.ovirt.org. It's connected to a special shared disk where the repos are stored, so it's easy to plug-unplug it from the vm if need upgrading or anything.</li>
  <li>proxy-phx-ovirt-org: This will be the local network squid proxy, is not yet functional but the idea is to use it to cache mostly yum packages, as we use intensive use of those when building with mock.</li>
</ul>

<h3 id="jenkins-vms">Jenkins VMs</h3>

<p>The jenkins DC has all the slaves and templates used to build them. The amount and oses/distros varies often but the organization should be quite stable.</p>

<p>The slaves are named following the pattern:</p>

<pre class="highlight plaintext"><code>${DISTRO}${VERSION}-vm${NUMBER}-phx-ovirt-org&#x000A;</code></pre>

<p>That way is fairly easy to know the relevant information about the slave just by it's name. The number is used only to distinguish between the vms from the same distro/version, so it's only requirement is to be unique, though we usually try to use the lowest available number (that might change in the future when we automate the slave creation, thatn might be replace with the build name or just a hash).</p>

<p>The templates are named the same way the slaves are, but instead of using the <code>vm${NUMBER}</code> suffix you only have two suffixes, <code>-base</code> and <code>-jenkins-slave</code>. The <code>-base</code> template (sometimes you'll see also a vm with that name, used to update the template) is a template you can use to build any server, it has only the base foreman hostgroup applied. The <code>-jenkins-slave</code> template has applied the jenkins-slave hostgroup.</p>

<p>Also keep in mind that puppet will be run again by the foreman finisher script when creating a new machine to make sure to apply the latest puppet manifests and configurations.</p>

<h2 id="tips-and-tricks">Tips and Tricks</h2>

<h3 id="strange-routingnetwork-issues">Strange routing/network issues</h3>

<p>For example, once saw that ping was unable to resolve any names, while dig/nslookup worked perfectly, that was caused by having wrong custom routing rules in a routing table aside from the main one, to see all the routing rules you can type:</p>

<blockquote>
  <p>ip route show table all</p>
</blockquote>

<p>Those were defined in the /etc/network-scripts/rules-ovirtmgmt file.</p>

<h3 id="vdsm-did-not-create-the-ovirtmgmt-libvirt-network">VDSM did not create the ovirtmgmt libvirt network</h3>

<p>In one of the hosts, after messing the network, vdsm did not automatically create the ovirtmgmt network in the libvirt setting, you can create it manually by:</p>

<pre class="highlight plaintext"><code>~# echo &amp;lt;&amp;lt;EOC &gt; ovirtmgmt_net.xml&#x000A;&lt;network&gt;&#x000A;  &lt;name&gt;vdsm-ovirtmgmt&lt;/name&gt;&#x000A;  &lt;forward mode='bridge'/&gt;&#x000A;  &lt;bridge name='ovirtmgmt'/&gt;&#x000A;  &lt;/network&gt;&#x000A;EOC&#x000A;~# virsh -c qemu:///system user: vdsm@ovirt pass: shibboleth&#x000A;(virsh)# net-create ovirtmgmt_net.xml # this creates the network in non-persistent mode, to force persistent we can&#x000A;                                      # just edit it and add a newline at the end&#x000A;(virsh)# net-edit ovirtmgmt&#x000A;(virsh)# net-autostart ovirtmgmt&#x000A;</code></pre>

<category:infrastructure>
</category:infrastructure>

</section>
</section>
</section>
<footer class='text-center' id='footer'>
<hr class='visible-print'>
<ul class='footer-nav-list'>
<li><a target="_blank" href="/privacy/">Privacy policy</a></li>
<li><a target="_blank" href="/about/">About</a></li>
<li><a target="_blank" href="/disclaimers/">Disclaimers</a></li>
</ul>

&copy; 2013&ndash;2015 oVirt
<div class='last-modified'>
Page last modified
Mon 22 Jun 2015 10:11 UTC
</div>
</footer>


<script src="/javascripts/application.js?1447609193" type="text/javascript"></script>

<!-- eloqua -->
<script src='http://www.redhat.com/j/elqNow/elqCfg.js' type='text/javascript'></script>
<script src='http://www.redhat.com/j/elqNow/elqImg.js' type='text/javascript'></script>

</body>
</html>
